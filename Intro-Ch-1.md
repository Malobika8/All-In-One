# Kafka
Apache Kafka isn't just a tool, it is a powerhouse reshaping how we navigate and manage data streams. 

## Agenda
A continuous stream of data orchestrated by a reactive Spring Boot producer seamlessly delivering messages to a Kafka Broker. Diligent consumer transcribing messages into DynamoDB.

### Message Broker
It's an intermediary software component responsible for facilitating communication & data exchange between different applications/systems. Its primary function is to decouple producers,
the applications that send data to brokers. Message broker acts as a mediator ensuring that messages are delivered efficiently from producers to consumers.

<img width="1062" alt="Screenshot 2024-08-24 at 12 44 50 PM" src="https://github.com/user-attachments/assets/46a198a2-72f9-497a-93c0-ae9fb8ac9873">

### Characteristics:
- Decoupling: Message Broker enables loose coupling between applications by allowing them to communicate without needing to be aware of each other's existence.
- The left-hand side sends messages to the broker & right-hand side consumes messages from the broker.
- Message Brokers often support asynchronous communication that allows producers & consumers to operate independently of each other's timing & availability.
- Scalability: Message brokers can handle large volumes of messages & scale horizontally to accommodate growing workloads
- Reliability: Reliability even in the case of system failures.

# Apache Kafka
It is a distributed, fault-tolerant & highly scalable message broker & stream processing platform. It was originally developed by LinkedIn & later open-sourced by an Apache software 
foundation project. It is designed to handle large volumes of data streams in real time in a fault-tolerant manner.

<img width="1022" alt="Screenshot 2024-08-24 at 1 16 48 PM" src="https://github.com/user-attachments/assets/02db82de-6424-4a74-8dd6-753ca2a23423">

### Key components of Kafka
- Producer: Application that publishes messages to a Kafka topic.
- Consumer: Application that subscribes to a Kafka topic & processes the published message.
- Broker: Core of the Kafka cluster. It stores & manages a stream of records.
- Topics: Topics in Kafka are used to categorize messages. Topics are divided into partitions allowing Kafka to parallelize processing & scale horizontally.
- ZooKeeper: Kafka relies on Apache ZooKeeper for distribution & management of the Kafka cluster. Its primary role is to manage the brokers. 

### Advantages
- Kafka can scale horizontally by adding more Brokers to the cluster providing high throughput & low latency in Data processing.
- Durability: Messages in Kafka are processed to disk providing their ability even during Node failure events.
- Fault Tolerant: It is designed to be tolerant ensuring that it can continue to operate seamlessly even during hardware or software failures.
- It allows for real-time stream processing making it suitable for applications that require low latency in data delivery.
- Decoupling: Kafka topic-based architecture enables the coupling between producers & consumers allowing flexibility & independence in application development
- Data Retention: Kafka provides configurable Data retention policies allowing organizations to retain messages for a specific period.
- Ecosystem integration. Kafka has a rich ecosystem with connectors for integrating with various data storage systems, stream processing frameworks & analytics tools. 

## Kafka Components

* Kafka Cluster:

  <img width="1058" alt="Screenshot 2024-08-24 at 1 36 18 PM" src="https://github.com/user-attachments/assets/cb0431cf-fc91-4fe7-bc1e-ef86e53da6b5">
  <img width="1088" alt="Screenshot 2024-08-24 at 1 45 55 PM" src="https://github.com/user-attachments/assets/1f22dbd7-d658-470d-a86e-8db7b9776721">

* Kafka Producer:

  <img width="1081" alt="Screenshot 2024-08-24 at 1 47 21 PM" src="https://github.com/user-attachments/assets/fcb05506-46f7-4287-a2c6-483fc55fe5c8">

* Kafka Consumer: Consumers can be part of a consumer group allowing them to parallelize the processing of messages.

  <img width="1083" alt="Screenshot 2024-08-24 at 1 48 27 PM" src="https://github.com/user-attachments/assets/90534f34-8494-41b7-82ba-f2021d31fa1a">

* Kafka Topics: In Apache Kafka, a topic is a fundamental abstraction that represents a category/feed name to which records/messages are published by producers & from which messages     are consumed by consumers. Topics play a crucial role in organizing/categorizing the flow of data within a Kafka cluster. They provide a way to structure & manage data streams         allowing or the separation of concerns in a distributed & scalable manner.

  <img width="1064" alt="Screenshot 2024-08-24 at 1 49 53 PM" src="https://github.com/user-attachments/assets/d255b55b-c489-4059-b0ec-e1f3a5236033">

  In Apache Kafka, a partition is the basic unit of parallelism & scalability. It is a way of horizontally dividing a topic into multiple independently managed units. Each partition is
  a strictly ordered linear sequence of records & it plays a crucial role in the distribution, parallel processing & fault tolerance of data within a Kafka cluster.

  <img width="1066" alt="Screenshot 2024-08-24 at 1 54 49 PM" src="https://github.com/user-attachments/assets/6e76f6c4-f36b-4302-9e12-9045af78d063">

  ### Key aspects of partitions in Kafka:

  Partitions enable parallel processing of data. Each partition can be thought of as an independent stream of messages. Producers can write & consumers can consume messages from         different partitions concurrently allowing Kafka to handle a high volume of data by distributing the workload across multiple partitions. Kafka achieves scalability by distributing
  partitions across multiple brokers & allows each broker to handle a subset of the partition. With an increase in data load, we can add more brokers to the Kafka cluster & the          partitions are automatically reassigned to LoadBalance & improve the throughput, and messages within a partition are strictly ordered based on their offset which is a unique           identifier assigned to each message in the partition. This ordering is guaranteed per partition but not across partitions. Once a message is written to a partition, it becomes part    of an immutable log. So, this characteristic simplifies data processing and ensures consistency within a partition

  Let's see how this offsetting works.

  When we talk about partitions, we also talk about offsets. So an offset is a unique identifier assigned to each message within a partition in a Kafka topic. It represents the          position or location of a message in the partition's log. So, offsets are used to track the process of consumers and enable them to resume consumption from a specific point even in    the event of failure or restart. 

  <img width="1068" alt="Screenshot 2024-08-24 at 8 01 37 PM" src="https://github.com/user-attachments/assets/3a0edf68-05bd-4573-919e-b94e098c69d8">

  Each message within a partition is assigned a monotonically increasing offset. So the offset is unique across partitions or topics. 

  *It's not unique across partitions but unique only within one single partition.* 

  Also, they have a sequential order. So, offsets are assigned in sequential order as messages are produced to a partition. The offset of a message is determined by its position in      the partition's log. Once assigned, the offset of a message is immutable. It does not change over time even if other messages are produced or consumed in the partition. 

  The consumers in CFA keep track of their process by maintaining the offset of the last processed message in each partition. This allows consumers to resume consumption from the        point where they left off ensuring that no messages are missed.

  Let's picture together how offsets work. When a producer sends a message to a topic, the message is appended to the log of the appropriate partition. So, the producer receives an      acknowledgment once the message is successfully written to the partition. At this point, the message is considered to have an offset.

  On the other hand, consumers track the offset at the last processed message in each partition. They consume from a consumer process message. It updates its offset to reflect the       position of the last successfully processed message. The offset is committed to a special Kafka topic called the "**Consumer Offsets**" topic. This topic stores the mapping between    consumer groups and their committed offsets. Now, in the event of a consumer restart or failure, the consumer uses the committed offset from the special CFA topic called "*consumer    offset*" to determine the last processed message for each partition. The consumer resumes consumption from the stored offset ensuring that it continues from the point of the last      successfully processed message. Consumers can write offsets periodically or after processing a batch of messages. This commit operation updates the stored offset or the Kafka          special offset called the consumer offset topic. Kafka provides different mechanisms for offset committing such as automatic committing or manual committing depending on the           consumer's configuration.

* Consumer Groups:

<img width="1114" alt="Screenshot 2024-08-24 at 8 23 27 PM" src="https://github.com/user-attachments/assets/85d7f15a-e350-4add-8aa6-f4ec07a13038">

Consumer groups are a mechanism designed to enable parallel and scalable processing of messages across multiple instances of a consumer application. Consumer groups    allow a set of consumers to work together to consume and process messages from one or more partitions of a topic. This concept is particularly useful for achieving high throughput      data processing and load balancing in distributed systems. A consumer group is a logical grouping of Kafka consumers that work together to consume and process messages from one or      more partitions of a topic. Each partition in a topic can be assigned to at most one consumer within a consumer group. This ensures that each message within a partition is processed    only by one consumer at a time. 

  Consumer groups are especially beneficial in scenarios where there is a need for parallel and scalable processing of large volumes of data & where high throughput data streams need to   be distributed and processed concurrently. Apart from that, it's also beneficial where load balancing across multiple consumers is essential for efficient resource utilization. 



now let's get started withApache Kafka and let's have a firsthandson so the first thing that we needto do just go to open the browser andnavigate to the URL Kafka apache.org orjust type in Google Apachi Kafka so thenopen the this link and here will land onthis landing page so here you see thatthis is a brief definition of Apachekfka saying that Apache gafka is anopen-source distributed event streamingplatform used by thousands of companiesfor highperformance data pipelinesstreaming analytics data integration andMission critical applications and then you can also have a lookat the rest of the website for us themost important part is to start ordownload Kafka but to do that just go toget started and then quick start so hereyou can also watch this video and thenjust scroll down the first step is is toget Kafka so in order to get Kafka clickon this download link it will navigateyou to a different page and here you candownload the recent or the most recentversion so for today at the time and thedate of recording this video so theversion is 2.13 360 all right so nowassuming that you clicked on this oneand downloaded already this ZIP filelet's go back and follow the rest of thesteps so the first thing that we need todo is to extract this zip folder so Ialready downloaded gafka and it'salready here so just double click inorder to unzip it and here just for mycase I will just call it Kafka uncoreserver just in order to be easier for meto navigate through the terminal or thecommand line so next we need to startthe Kafka environment so in order to dothat we need to start our zookeeper soin order to do that we need our terminaland we need to navigate to The Pathwhere we extracted Apache kfka andsimply run thiscommand all right so on the terminalmake sure that you are on the rootfolder where you extracted the CFAserver and to make sure just run thecommand LS and here we see that we havethe zip file and also the folder whereextracted so first let's navigate to theKafka server folder and in here we willrun the First Command that we have inthis documentation so I will just copythis one and then I will run it in orderto start the Zookeeper service first solet's space it here and now just in caseyou want to see what do we have insidethis Kafka server so here we have thisbin folder where we have all the shellscripts all the scripts and also herefor the windows users you have thisWindows file folder sorry so in thisfolder also you have all the best thebad scripts so you can run them insteadso this one this beenin folder iscompatible with Mac and Linux and alsofor Windows users you need to navigateto the windows folder in order to beable to run these command so you need tochange your path accordingly dependingon your operating system all right sonow let's go ahead and run this commandand see what will happen so here we needto make sure that everything startedwithout any exceptions without anyerrors which is the case that we haveright here so we don't have anyexceptions and if everything is up andrunning with the zooe keeper now let'sgo back to the documentation and let'scheck the next step so then we need toopen another terminal session and runthis command so I will go back to myterminal and I will open a new tab oryou can also open a new terminalinstance and run this one so here onthis instance I'm also again in theKafka server folder so I will paste thecommand to run the Kafka servers so hitenter and here let's make sure thateverything is up and running so as youcan see from the logs right hereeverything is up and running and wedon't have any exceptions in case forexample if the port is already used orif you have another instance running andso on so forth so you will face anexception and depending on the exceptiontry to solve it and just in case you youcan solve it just baste in the commentscomment down below or just paste yourissue on our Discord server and youabsolutely will get help so as you cansee here the Kafka server is alreadystarted on the port 9092 which is thedefault port for the Apache Kafka brokerso now let's explore a little bit moreKafka so we will go back to thedocumentation in here so then we cancreate a topic to store uh events so allwe need to do is to copy this Commandright here so I will copy it and then Iwill open a new tab in my terminal andrun this command so I will paste thiscommand so this gafka topics and then wehave this this argument create and youwant to create a topic let's call itquick start event and then we need toprecise the bootstrap server which isour local host 1992 so pay attention tothese arguments because we will needthem later on when we start our springapplication so let's hit enter in hereso here it says that created topic QuickStart start event so now let's go backand see what are the next steps in orderto publish some events and to consumethem later on so here as a next step wecan also describe the different topicsthat we have so let's copy also thiscommand and see what are the topics thatwe have so for example here I willcreate another topic and I will call itso for example I will call it test testtopic and then I will describe or listthe list of the topics that we have inour broker so now I will paste thecommand that I already copied so Kafkatopics and then describe the topics andhere so we want to describe for examplethe quick start event and if I hit enterit will give us the details of thisevent if I run this command again andput the test topic it will give us alsothe information of our test topic or thesecond topic that we created all rightafter that let's try to write some eventor to push some messages to our brokerby pushing them to this topic or thequick start events topic so let's goback to the documentation and followalong so here to write some events intothe topic we already have a scriptproviding provided Us by Kafka that wecan use to publish some events to sometopic and as you can see here so we havethis command I will copy it here andthen let's explore it together in thecommandline so now if I paste this one you willsee that we have this command so it'susing the CFA console producer sh sothis is the script and we want to topublish to the quick start events sothis is our topic and here we have ourbootstrap server which is the one thatwe just started before so I will hitenter and then we will be prompted andnow we are able or will be able topublish any event so here hellothis is my first message for example andthen hit enter so this one was alreadycued to the Kafka topics now let's goback to the documentation and see how wecan consume a topic so now in order toread the events I will copy thisone and then I will open a new tab in myterminal and I will consume it because Idon't want to exit this one I willpublish other message in just fewseconds all right so now in this new tabas you can see here we have also ascript it's called Kafka consoleconsumer and here we we need to precisethe topic which is the Quick Startevents the one that we already publishedon it in here so also from beginning wewant to start consuming from thebeginning and here we have the bootstrapserver of course that we need to preciseso now if I hit enter so you see thathere we have hello this is my firstmessage and this consumer is alreadylistening so in casee I just type a newmessage so this is a new message and Ihit enter and if I go back here you willsee that it's already displayed in hereso you can have your terminal like bothinstances one next to the other and onceyou push an event here so hello again Iwill try to go fast to show you so hereI publish and now it will pop up as youcan see in here so this is how we canpublish and we can how we can consumealso an event so all this is already inthe quick start also you can followalong and you can see the differentother the other different options thatyou can use within from or from thisdocumentation now let's move on to howto see how we can implement or how wecan use kfka within a spring bootapplication and finally don't miss outthe rest of the video because we will beimplementing a really nice applicationusing spring boot reactive and we willbe consuming a reactive rest API and wewill be publishing everything to ourApache server and consuming the messagesfrom there all right now let's go aheadand create a new spring boot project soin order to do that just go to thespring initializer or you can also useyour IDE anj or eclipse in order tocreate a new spring boot project so herefirst I will choose a maven project Javalanguage and then the latest versionwhich is 315 and now I will provide thegroup ID which is com. alibu and thenthe artifact I will say just Kafka Dasdemo all right so here the name would beKafka demo and here spring demo projectfor spring Boot and Kafka and here Iwill just call the package Kafka insteadof Kafka demo all right so now we needto add few dependencies first let'sstart with Lum book and then we needneed spring web in order to expose ourrest API and to be able to send andconsume messages and finally we need tosearch for Kafka and here you have twodependencies choose the first one whichis spring for Apache Kafka all right sonow you have your project go ahead clickon generate and open it within your IDEall right so now I have my project openand ready so I opted for the monor repoapproach and here you see that you havethe scafa demo project and then we willbe adding other projects when we move tothe final application demo that we willImplement together so here for the Kafkademo here let's go first and check ourresource file so here we have ourproperties and you know I prefer usingyaml representation so I will justswitch it to yaml and now let's add fewconfiguration first we need to configurethe Kafka server so here here we havespring. cafka and then we have consumerso we want to give the configuration forour consumer first so and then we needour bootstrap servers so here as you cansee it's we can provide multiple serversnot only one and for that remember fromthe command line it was Local Host andthen the port9092 all right so next as you rememberwhen we explained in the in this diagramwe said that a consumer is a part of aconsumer group so now we need to tellwhich consumer group we want to createso we can create multiple consumergroups but for now let's just go aheadand create a group ID and here let'sgive it for example my group just forstart then we also spoke about offsetswe spoke about partitions and so on soforth so now now also we need to giveanother property in order to tell springwhat we need to do when for example welose the offset or we want to reset theoffset so here we have a property calledAuto offset reset so here we want to saywhat we want to do in case we lose thereset or we want to reset our offset sohere we you see that we have severaloptions we will be using the first onewhich is the earliest and as you can seehere see it automatically resets theoffset to the earliest offset and herewe also have exception so it will bethrown an exception to the consumerlatest will we automatically reset theoffset to the latest offset and N if wedon't want to opt for a reset offset solet's choose earliest in here and thenas you can see in here so this is likethe update of our offset so this is whatwe made and now in order to consume amessage so the message will beserialized from the pro producer andwhen we when it comes to the consumer weneed to deserialize the message so wehave serialization from the producerpart and Des serialization from theconsumer part so this is what we will beproviding as properties right now sohere let's say we have a keydeserializer so like because the messagewill be like a key value so we will havea key and a value so the key in thiscase we will use the org. aachedoka and then we have common it's apackage called common and thenserialization and then dot so for nowlike we will start with a simple examplelike we will be sending strings andconsuming strings so we will be using aclass called string D serializer allright so let's duplicate this line sohere as we can see we have keyserializer now we need to provide thevalue serializer the serializer sorry sohere the same way we will be using thesame deserializer since as I mentionedwe will be in the first example sendingonly strings and then we will seeanother example if we want to send andreceive Json objects for example nowthis is the configuration for ourconsumer let's go ahead and configureour producer so for the producer also weneed to tell what is the bootstrap orwhat are the bootstrap servers so heredon't worry in this in this demo projectwe it will be the producer and theconsumer at the same time but later onwe will be creating a full applicationwhere we will be implementing anapplication and we will have anapplication as a producer and anotherapplication as a consumer so now it'sjust for demo just to get you startedwith Apache Kafka so here imagine likenow we are on the producer project orthe producer application and we areconfiguring the producer so here thispart you put it only on the consumerproject and this part you put it on theproducer project so here let's go backand finish the implementation for ourproducer so in the same way here we needto tell or we need to provide the bostrap servers which is the same one asin here since we will be sending to thesame broker and then also we need twoproperties these ones so I will copythem and then I will paste it so we havethe key now it's not serializer but it'sdeserializer all right uh sorry it's theother way around it's not deserializerbut it's a serializer as I mentionedbefore when we send an event or when theproducer sends an event or pushes amessage it needs to be serialized andthe consumer needs to serialize it todeserialize it so now so it's the keyserializer and the key and the valueserializer and here you need to becareful about this point so for examplethe prod the producer if the theproducer sends a string you also need tohave a this a string der serializer butjust you cannot for example send uhanother type of data serialized and thenyou try to deserialize using a differentder serializer all right so now we haveour configuration for our consumer andour producer so let's we can just tryand click start the application let'senable uh annotation processing forlombok and here we just need to makesure that the application is up andrunning which is the case right here sofor example here we did not provide forexample something wrong and here just weneed to pay attention to some propertyas we mentioned here that it's a keyserializer also this one needs to bestring serializer not D serializer soalso you need to be careful about thispoint right here so I will stop theapplication and now let's move on to thenext part after providing the CFAproperties the first thing that we needto do we need to create our topic so thefirst thing let's right click here Javaclass and I will put it in a configpackage and then I will call it Kafkatopic config so Kafka topic config sohere so this one in order to make this aconfiguration as you know within springboot we need to mark it as configurationand then what we need to do we need toprovide a bean of type Topic in order tocreate a topic so here let's use the beannotation and then let's create ourBean which is will be of type new topicso this means that we want to create anew topic in our Apache Kafka messagebroker so let's call it for examplealibu topic or my topic or you can callthis Bean wherever you want so now weneed to provide or we need to return anobject of topic Builder so we have aclass called topic Builder and then wejust give it the name so this will willbe the name of the topic and here let'scall it alibu for example and withinthis Builder we can also provide otherinformation and among these informationssince we said and as explained before atopic is already composed of severalpartitions so here you can say forexample I want to have uh for examplelet's say five partitions also we canhave some replicas and so on so forthbut this will be our own configurationand also we can leave all the to Kafkain order to have the defaultconfiguration from Kafka so then we justneed to provide the name in order tobuild a default topic we just need toprovide the name and then call the buildmethod so this way we just created a newtopic called alibu or also you can callit for example alibu topic or anythingyou want so the next step is is tocreate a CFA producer so we need tocreate the producer class that will sendor that will cue a message to ourbroker now I will right click in hereand create a new class and I will createa package called producer and then Iwill create a class called it Kafkaproducer and now what we need to dofirst we need to mark this class as aspring component or spring Bean so wecan use for example the serviceannotation or we can also use thecomponent annotation or any other customannotation you want to create also Iwill be using the required Constructorin order to have a Constructor withparameters or with the requiredparameters that I want to so the firstthing we need to do is we want to have aprivate final and then we have an objector a bean called CFA template so thisKafka template takes is already ageneric one and it takes let's downloadthe source and as you can see here ittakes a key and a value so here the keytype and the value type and this one ifif you remember the configuration thatwe created together for our producerwhich is the key serializer and thevalue serializer so now we need toprovide the same object well like twoobjects key value of type string so thisis how it works I will show you later onwhen we want to use Json for example sohere it will be the key is of typestring and also the value will be oftype string let's call it Kafka templateand now so we don't need to createmanually the Constructor since it willbe provided by this annotation so now Iwant to create a public void so I willcall it for example send message orpublish message you can call it anythingyou want and for this I will ask to geta parameter uh of type string and I willcall it message and then all I need todo is to call my kfka template and thensend so this send we have like severalimplementation or several overridemethod as you can see here we canwhether send a message or topic and themessage produce or record and so and soforth so for our case we need to provideto which topic we want to send ourmessage and then the object or themessage we want to publish so the topicit's alibu as mentioned here in thisconfiguration so you need to be carefuldon't make typos otherwise you mighthave exceptions so here just before thatI will inject here my annotation slf4jin order to have a logger and I want tolog. info and then I will use string.format and then for example let's saysending or message send for exampleSending message to alibu topic and herelet's say for example this is ourmessage and it will be percent s andhere we can concatenate with MSG also ifyou want to we can also add staticimport to this one so here you know thatthis format is coming from the stringclass so now we have our producer sothis producer is able to send or to c amessage to our Apache Kafka server allright now let's go ahead and create awrist controller in order to be able tocall this Kafka producer so I willcreate a new class and I will call itfor example in rest or you can call itcontroller whatever you want just feelfree to name your packages and classesthe way you want so here I will justcall it message controller all right sohere it will be rest controller and thenI want to have a um a required RSConstructor and also I want to have arequest mapping for my controller so Iwill call it slash API and thenslv1 and for example I would saymessages so here I will need my privatefinal Kafka producer and then let's callit Kafka producer or just producer andthen I will create just a post mappingsince we want to post something so herewe have a post mapping and then it willbe a public response entity and let'ssay for example of type string so here Iwill say for example sendmessage and then let's ask for examplefor a request body which will be just astring and then let's call itmessage so for our message then what wewant to do all we need to do is firstcalling our kafa producer and then sendmessage and let's pass the message as aparameter and then let's return aresponse entity do okay for example andhere we can say for example message cueand we can add it for examplesuccessfully all right so here we haveour message and we have our restcontroller now before going and movingto testing let's just try to start ourapplication and see what will happen ifeverything is fine or if the applicationstarts or not so here we see that wehave a bunch of extra logs so this iscoming from the configuration that weprovided for our Kafka and also fromcreating the topic the configuration tocreate the new topic so as you can seehere we have the Boost servers we havethe configuration that we provided andalso this is the configuration of thedefault configuration from Kafka so ifwe scroll to the bottom here we see thatthe application already started nowlet's go ahead and test this applicationor test this small IPI so before we moveon and start testing let's first openour terminal and now let's navigate toour server so here I will just navigateto our Kafka server and now let's goback to the documentation and as you cansee here in order to read an event weneed to run this command line I willcopy it and then I will paste it in hereso let's change the topic name since ourtopic is called now alibu the one wejust created within our application andlet's hit enter so far we don't have anymessages consumed or queued to ourapplication so now let's open Postmanand let's send the first message so hereI prepared already the request for youso all we need to call is HTTP and thenour application is already started onthe port 8080 and here our API is sl/API slv1 SL messages and here we want tosend a post request so we need toprovide a body so our body will be oftype Pro or will be row and the type orthe content will be of type string sohere let's say this is my firstmessage to Apache Kafka all right andthen let's click on send and see whatwill happen so we have 200 in here andalso we have the message queuedsuccessfully now if I go back to ouranell so we see here that our consumeralready consumed this message so againif we go back and we send anothermessage so let's say for example here isa newmessage and we hit or we click on sendand we go back again to our terminal wesee that we have the message right herehere so this way we are able to send amessage to our queue from our producernow let's move on and let's Implement aa real consumer for our topic all rightnow let's create our consumer first ofall I will stop this consumer fromconsuming the messages and I will alsostop the application so now let's openhere and let's create a new class and ina new package I will call it consumerand then I will call call it for exampleKafka consumer as my consumer class nameso this one of course we need to mark itas a spring component so let's give itthe service annotation and then all Ineed to do is to create a public void sothis is our method and then let's callit consume message for examp for exampleso let's call it consume MSG and thenwhat we want to consume so we arealready publishing or pushing a string mmessage so we need to consume a stringmessage all right so now before thatlet's use our slf for J annotation inorder to have our logger so let's sayour log.info and here for example let's useagain our string do format method andhere let's say consuming the messagefrom alibutopic and let's say here let me add thisspace and here let's also concatenateour message so percent s and here itwill be MSG all right we can alsostatically import this one and now whatI need to do is simply add an annotationin order to tell spring that this is aKafka listener so as I mentioned thename is already Kafka listener So withinthis kafa listener we need to provide abunch of information well not a bunch ofthem but at least we need to provide twoor one minimum information so theminimal one is the topics what are thetopics that we want to consume and inthis case we want to consume our topiccalled or we named it alibu when wecreated our topic configuration and thensince here in this configuration file wesaid that we want to have a group ID andwe called it my group so here we we canalso provide this this information tothis Kafka listener annotation so herewe have a group ID and let's say thatthis listener or this consumer is partof this group ID all right so now wejust want to consum a message and seethat this message is just running so nowjust make sure again our consumer theone the one provided by Apache kfka isalready stopped and now if I run theapplication we will see that theconsumer or this consumer will belogging this information or the consumedmessages so let's start ourapplication and see what willhappen so all right so here we have ourapplication running and we already seesome logging from our Kafka consumer sohere it says consuming the message fromalibu topic so this one this is thefirst message and here is the newmessage so now let's send anothermessage so this is a message to theconsumer and let's click on send now ifI go back to the browser we will seethat we have a new message logged hereso here we have consuming the messageand this is the message to the consumerif we send another one it will be alsologged in here so now this is how we cancreate a consumer for ourapplication all right so now what if wewant to send Json data format so firstof all Apache kfka stores and transportsdata in a bite format so there arenumbers of buil-in serializers anddeserializers but it does not includeany for Json and the reason that CFAdoes not provide a Json serializer is toavoid imposing a specific serializationformat on users so kfka is designed tobe agnostic to the data formats used byproducers and consumers allowingflexibility in accommodating variousdata structures and serializationformats so butspring Kafka created a Json serializerand Json derial deserializer which canwe use to convert Java object to andfrom Json so what we will see so we willsend a Java object as Json format fromthe consumer and it will from theproducer sorry and it will be consumedby the consumer as also a Json format sowe will see how we can configure ourapplication or our consumer and producerin order to send Json and to receiveJson so the first thing we need to do weneed to go and update the configurationso let's open our application yaml righthere and here where we have thisdeserializer for the consumer and theserializer for the producer we need tointroduce a small change right here sothe change is we need to tell ourapplication which serializer and Dserializer we want so I will justduplicate this one and comment out thisline so here instead of using the org.Apache and so on so forth we haveanother implementation we say org.springframework do cafka do support doserializer dot we have a class calledJson D serializer so this is the classfor our D serializer we will do the samefor the serializer so I will duplicatethis line comment out the old one andjust change this this path so this oneshould be Json serializer let me removethis and here we need to talk aboutserializer not D serializer and ofcourse don't forget to remove the D fromhere so it's value serializer all rightso next what we want to do we want tocreate an object that we want to sendfrom our consumer uh from our producerand consume from our consumer so here Iwill just create a new class in apackage I will call it payload becausewhen we send something we call itpayload and I we call it for examplestudent so I will send a student objecthaving for example private int ID andfor example private string first nameand last name so this one also last nameand we can use the lombok annotations tocreate Getters and also Setters and weneed the all arcs Constructor and noarcs Constructor but we can skip thatsince it's already we have our defaultConstructor there so now the next stepis let's see how we can or like do weneed any configuration to send andreceive Json format the the answer isjust in few seconds so after changingthe configuration the next step is is tochange our producer so here we have theexample for the string producer let'skeep it that way and I will create a newone so I will create a new class and Iwill call it Kafka Json producer so likeyou can have and you will have the wholecode for everything so the first thingof course this is a service and I needmy required arcs Constructor in order toinject my dependencies so again we needour Kafka template so here I will doprivate final CFA template and now itwill be a string for the key and ourobject as the value so our object is thestudent so it will be student let's callit again cfat template and now I need apublic void and let let's call it sendmessage so to send a message what weneed we need our student as object solet's call a student or simply data orlet's call it simply student all rightso after that what we need to do we justas we did with the Kafka producer justCFA template. send but it's not the caseso here what we need to do we need tocreate an object of type message so hereI will create an object of type messageand it's coming from the kfka package sowe need to be careful about this so thismessage will be of type student and thenI will call it message equals messageBuilder Dot and then we have a methodcalled Write payload sorry not write butwith payload and I guess I imported thismessage from the wrong package but we wewill see so here we have with uh payloadand our payload is the student object sonext we have to set the header so we canset header or headers so our header isthe Kafka topic so we have a classcalled Kafkaheaders dotopic so this is the topic or like theobject that we want to send we want tospecify with to which topic we want tosend this one and our topic as wementioned before is called alibu andthen do build all right so here thisbuild method it will return object oftype student and here we can make sureor check which package is this one soyeah when we imported our message classwe imported it from the wrong place soit should be orgspring framework.messaging not the one from Kafka solet's change this one so we can removethis import from here and we can importthe new new one so it's the correct oneis springf framework. messaging and asyou can see it's a generic type so inthis way we are able to send our messagebut here we just built the message weneed now to send it so now we need tocall our cka temp plate do send and asyou can see the send it also like wehave an send method that takes a messageas an object so here we want to send ourmessage all right so this is the methodthat we are talking about it's acompletable future and it takes themessage as an object all right so thisis the send method now let's move on tothe next step all right now let's adjustour controller so the first thing herewe have one kfka producer which is aservice and we have the new one which isalso ajacent the kfka Json producerwhich is also a service so when I comehere to the controller we might facelike an issue we like we cannot know orwe don't know which one to inject buthere we have like uh the Kafka producerso here we just need to change this oneto the Kafka Json producer and consumethat one all right so now I will adjustthis method so I will just duplicatethis method here and I will call it sendJson message and here instead I will askor I will require a student object andhere for this send message I will justinject thenew Kafka Json producer and I will callit Kafka Json producer all right so nowinstead of using the kafa producer let'suse the Kafka Json producer and let'ssend the message so here also we canreturn send CED successfully and let'ssay as Json just to differentiatebetween the two different method methodsso here we have a post mapping and now Iwill just add a small thing just todifferentiate it I will just add/ Jsonto know that we are calling the JsonMethod All right so like this is justfor a test it's it's not the best waylike to inject to services like doingalmost the same thing we need toseparate them or like use one of thembut it's just for the purpose oflearning so now we even have our ourconsumer right here so which consumethis topic and now we can just go aheadstart the application tested and we willsee how this message will be displayedso it it will be will it be displayed asa string or will it pose an issue sincewe have a Json D serializer so let's goahead and check that one otherwise wecan also create or change our Kafkalistener accordingly all right now let'sstart our application and test again andsee as you mentioned before if our appapplication or our consumer is able tohandle these messages so first we seethat we have some exceptions whilestarting the application so let'sinvestigate this exception so it startsfrom here so we have like um er errorHunter cannot process the serializationexception let's check the cost by andhere it says or the serialized key valuefor the partition alipo zero and so onand so forth so if we scroll also downwe see here that we have a differentcuse so it says that the class com.alibu Kafka payload student is not inthe trusted packages so this means soand here it mentioned that if youbelieve this class is safe to disrealize please provide its name if thederial deserialization is only done bytrusted Source you can also enable trustall with star so for that we need to addone property here to our application inorder to tell Kafka that we trust thesepackages and we want to dis realize fromthem and specifying a trusted packagesin Kafka is a security measure toprevent certain types of attacks relatedto deserialization so the Der serialdeserialization process uh is theprocess of converting data in aserialized format such as Json forexample back into its uh origin objectfrom so the process can be exploited ifnot properly secured that's why we gotthis problem and now to fix it we justneed to add one property so here withinwithin the consumer we can sayproperties and then we have one propertycalled spring.Json dot trusted packages trusted andthe then Daspackages and here we can we just providethe package as mentioned in here so itsays that this package is not trusted sowe can whether provide a single value orlike a list like we can provide multiplevalues or multiple packages or if youwant you can allow or trust everythingby providing a star in uh in uh incommas in colins sorry not commas but uhstar in columns and now we will be ableto trust this package so so let's startour application again and see if thisproperty works so I will start theapplication and now let's see if we havesomething different so yes we havesomething different and here if youscroll down to the logs we will see thatwe have now this is our D serializer andalso now we have a different exceptionso let's go down to the cast by and herewe see that cannot convert from studentto string so this means that you are notable or our consumer is not able toderealize or to transform the Jsonobject to a string object so we need toadjust our Kafka listener so let's goahead and do it so now within our Kafkalistener we can whether Just Adjust thismethod or create a new listener so wecan create another one and I will justcomment out this annotation I just wantdon't I just don't want to use this oneso here I just call it consume Jsonmessage and now all we need to do is toprovide the type so here it's of typestudent and let me rename this one andnow I'm going to call it student so hereautomatically spring will be able todeserialize the object that we willreceive into a student object and printit in here so here let's just also callthe two string method and that's it sothis is how we can adjust our this ishow we adjust our Kafka consumer so alsoI could just create a Kafka Jsonconsumer and like listening to the sametopic or even going here to our Kafkatopic and create a new topic for examplecolonate alibu Json in order to consumeit and of course we need to adjust ourJson producer to send to this new topicbut I just wanted to keep it the sameway so just here to explain to you thatyou have multiple options so now comingback to this one so here this is thesmall change and now if I restart theapplication and see the changes or theimpact so here let's close this one andnow I will go back to postman and sendagain one message so if I open back myconsole so here we should see somethingso consuming the message from alibutopic and here so this is the object sohere it's just because I don't have andimplemented two string method in mystudent class so if you want we can dothat quickly so here we can also callthe two string method from lumbo orannotation sorry and now if I restartthe application and send again anothermessage so if I click on send and I goback here we see here that we have theobject or the object just get impprinted so if I send again another oneso so let's say John forexample and here John do and if I clickon send we should also see again that wehave this joho getting printed righthere all right so now our consumer worksso now the best parts begin right now sonow we will move to the implementationof our real world application so our orthe real word example and we will beimplementing really cool applicationusing uh spring reactive all right nowthe best part begins so this is a globaloverview of the project that we will bebuilding together so let's start fromthe left hand side so first we willcreate a spring boot project and thiswill be our producer and this producerwill have an API where we can invoke oreven start consuming a stream of datacoming from from a reactive rest API sothis reactive rest API is the stream ofthe wikip media and I will give you anoverview also about Wikipedia in justfew moments so also you can see the linkis in here and I will also put it in thedescription of this video so here ourrest API or our reactive consumer willconsume these data so here we have aconsumer of our reactive rest API comingfrom Wikipedia and this rest API or thisconsumer will call the producer thatwill publish all these events to ourKafkacluster so this is the cluster and wehave like the Brokers topics partitionsand so and so forth all the things thatI already explained before and from theother side like the the consumer side wewill also have another spring bootproject and we will this will be ourconsumer so the consumer will subscribeand listen to all the messages coming toa specific topic all right and thenafterwards here I will just notimplement this part I will just belogging the data but here feel free todo whatever you want with the data youhave here so you can forexample puras the data to uh mongodbpostre SQL an S3 bucket or any otherstorage system all right so so here likeI will leave you the the chance and alsoI would like to see what you will bedoing and how you will Implement thatand I would love to see your comments onthe video and also if you want justpublish the code on our Discord serverand I will be happy to review it andgive you my feedback all right so alsohere you can picture these two servicesor these two projects as twomicroservices communicating togetherthrough a Kafka broker all right so nowlet's go to this one so before first ofall let me give you an overview what isWikipedia I think Wikipedia is somethingthat's related to you when we sayWikipedia so Wikipedia is a group of allthese Services we have right here sothis is like Wikipedia is a globalmovement whose mission is to bring freeeducational content to the world so likeit's uh Wikipedia Wiki uh wiki sharywiki books and so and and so forth andthis stream or this stream wikip mediais a stream that represents all thechanges that are made by the communityon the Wikipedia projects all right andin order to see that so here we havethis link I will paste it in here andnow I will just refresh the page so yousee here so if I scroll down it's astream of data so every time I scroll Iwill see data so if you want also tounderstand and to have a look aboutspring reactive or like reactive uh restapis I would love to invite you to checkthe spring reactive course that Ipublished before on my channel so I willleave you also the link in thedescription of this video so here we seethe data maybe we can also zoom in a bitand see what are the the data is so herewe have the topic we have the schema andwe have so many information we can alsogo and see this in details lateron so just to give you an idea about thestream and the size of the stream thatwe will receive from these endpoint sothere is an open project it's called Iwill also leave you the link in thedescription of this video where you cansee an overview so here we see that wehave a stream of68,000 and it's still growing so this isa huge stream and in order to be able toprocess that we need also a reallystrong and scalable and Powerful messagebroker like Apache Kafka so here this isa dashboard and you can have a look onit to see the all the data and what arethe streams so here for example we haveall these events we have the size of theedit and we have how many records aredisplayed so so far like Since switchingto this tab you see that we we got morethan 1,000 records all right all rightso this will be the parts or thebuilding blocks of our application nowlet's move on to the next part and startbuilding this project all right so nowlet's go ahead and create and configureour two projects so first it will be amaven project and here for the group IDit will be com. alibu Wikimedia forexample and now to differentiate the twoprojects here for the artifact I willadd this one for the producer and I willkeep it this way so now let's go aheadand add our dependencies so first weneed our Apachi kfka and then we needour reactive web so here we will not beusing classic spring uh web annotationbut we need a spring reactive in orderto be able to consume a reactive API andthen of course we need our lombokannotation so this is the first projectlet's go ahead click on generatedownload it and also import it in yourIDE and let's also create the secondproject all right so after downloadingthe first project now let's change thisone to Consumer so this will be ourconsumer project so for this one itlet's remove the spring reactive webmaybe let's have for example just springweb in order if we want to add some APIbut for now it's not the plan but maybelater on all right so this is our secondproject let's click on generate andlet's open both of the projects in ourntity J and start coding all right so aswe did before the first thing that wewill do is to just first rename this oneto yaml property and now we can gofaster because we can go ahead and copysome properties from this one so herelet's copy these properties from theproducerand then we need to add spring. CFA andthen in this way so so since we areworking on the producer just to bringyou back since we are working on theproducer we can copy paste this one andalso what we want to produce we want toproduce a string instead of Json becauselike I just don't want to waste yourtime and creating an object and disrealizing uh the the stream or the datathat you will receive I will justinstead send it as a string so firstthis is the configuration of ourproducer what we need to do next is tocreate our topic configuration so now Iwill just right click and then inside aconfig package I will say for exampleWikipedia topic config all right so inthe same way if you want we can also goahead and copy this Bean so it's simpleeven to rewrite it our own but let'sjust make it faster and copy this beanfrom here and paste it in here and alsodon't forget to add the ad configurationannotation there and now let's call itWikipedia and then Dash topic orWikipedia stream so let's call it thisway all right so now we have our topicready here let's also rename this oneWikipedia topic or Wikipedia streamtopic and then let's move on to the nextstep which is consuming our stream APIso so now within our producer projectagain in the configuration and in orderto be able to consume a reactive APIfirst we need to have some configurationso I will create a configuration classand I will call it web config or webclient config to be more consistent sothis and here in order to make this onea configuration we need to add add theconfiguration annotation and now I wantto create a bean and this Bean will beof type web client. Builder all right sothis web client and then we have abuilder so let's call it web clientBuilder so web and here we simply needto return web clients and then doBuilder so this is all we need to do nowlet's move on and create our web clientconsumer or our stream consumer allright now let's go ahead and create ourproducer so here right click new andthen producer and then let's call it forexample Wikipediaproducer so this producer class will bethe class as we did before is just theclass to send a topic to our Apache kfkaso here we can also go back to our CFAdemu and here let's go to the Java andthen producer and we need or we can takethis from the Kafka producer so here wejust can copy this one and also we cancopy this and let's go back to ourWikipedia producer so we can close thisclass and now I will paste theannotations and then I will paste thecode that I copied so here like forexample that we can whether change thelog or we can also remove this one fornow we don't need it and here we need torename name the topic to ourWikipedia and then stream all right soalso just to be sure and over typos Iwill recommend so let's close the demo Irecommend that you go here to the topicconfig and just copy and paste the namejust to avoid these typos also you cancreate a variable for that and use iteverywhere all right so this is not thatimportant but just you need to make surethat you don't make typos just in caseall right so now we have our producerready now let's move on and create thestream consumer service the one thatwill consume the stream API and then itwill publish the message to the Kafkatopic all right now I will right clickhere and create a new class and I willcall it for example the stream orWikipedia or you can call the packagewhatever you want and here I will callit Wiki media stream consumer so thisone will consume to our stream comingfrom the Wikipedia let's make this onefull screen and first of all we need tomake this a service and then let's sayfor example slf for J just we want tolog what we have all right then what weneed to do we need to inject our webclient so private final and then webclient object let's call it web clientand then we need of course our privatefinal and then we need our Wikipediaproducer so let's call it producer andnow let's create uh a Constructor withthese two parameters in order to injectthem now we need to make some smallchanges so here instead of the webclient we need web client. Builder allright so because this is the this is thebean that we created in ourconfiguration so here instead what wewant to do this web client equals thisBuilder so let's also rename this one toBuilder to be more consistent and hereequals web client Builder and then dobase URL so we need to set our base URLso the base URL for us is the stream.wikimedia.org slv2 so this is our baseURL and then we need to call of courseof course the build method so let memaybe break this down so you can seeeverything at a glans so this is how wecan configure or how we can configureour web client object so now the nextthing we need is we want to create amethod so it will be a public voidconsume so let's call it consume streamandpublish all right so this consume streamand publish will consume the streamcoming from this API and then publish itto our producer or to our Kafka brokerso what we need to do here we have ourweb client and then it's a get method sodo get and then we need to pass the URIso the rest of the URL that you want toconsume so it's slash stream and then SLrecent changes or recent change allright so this is the end point and andthen what we want to do is to retrieveright and then we want it to BU to fluxso we want to transform our body to fluxand the format is string right so thisis what we decided we said okay we wantto consume and to produce a string andafterwards how we can still consume whatis coming from here so it's the publishsubscribe pattern so that's correct weneed to subscribe rbe to it so aftersubscribing let's maybe try a system andthen do out just to print or uh let'ssay for example log and then info justto log what we have in here since weinjected our logger here all right andthen we will change this one to send thedata to the producer so what what comesnext is we need to create a rest APIwhich all allow us and help us just totrigger this this consumption of the APIand sending the data so let's go aheadand do it or you can also do it in inthis place by injecting a command lineRunner bean and just trigger this consconsumer but let's make it as a rest APIthat will be better perfect now let'sright click here and let's create a newclass let's call it maybe rest and thenlet's say for example Wiki mediacontroller all right so this one I wantit to be restcontroller and also I want my requiredAR Constructor and my request mapping sothe request mapping I will call it/ Apislv1 SL wikii media just to make itsimple like that and then what we needto do we need our consumer so let'sinject privatefinal our wikii media stream consumerlet's call it consumer or let's call itstream consumer to be more consistent sonow all I need to do is to have a getmapping and then public void and thenlet's say for example uh startconsumption or start publishing since wethe main goal is to publish this streamto our CFA so uh start publishing andthen we require nothing so here let'smake it a void or you can also sayresponse entity of type void or of anytype and then just return some messageor some confirmation all right so butthe most important here is we want tocall our stream consumer. consum streamand publish all right so this is our APIwhat we need to do or what we want to donext is to start the the Wikipediaproducer application and we need to seethat we are consuming the stream fromour Downstream API and we need we needit to be logged in our output console solet's go ahead and do that all right soto do that whether you can click here onproducer application and run it fromhere or also within anj you have theoption services and then you have allthe spring boot applications availablein your workspace or in your projectright right here so for our case we wantto start the producer application and Iwill start it in debugmode just in case if something happens Iwant to go back and be able to debug soalso you see that the configuration forour Kafka is is working and here we seethat we have also this output for Kafkaso first I will clean this one and thenwhat I will be doing I will open Postmanand send a get request to this API let'slet's just say change the url right hereand it will be a get mapping so we havenothing in here and I will just clickokay so it's 404 because we have a typohere and let's click Send again so it's200 now if I go back to our console Iexpect to see that we have a stream ofdata so this is what we see so we havelike the Wikipedia stream uh consumerand it's consuming this API so as youcan see as I scroll down it willcontinue printing the data so we arestreaming the data or we are reading orsubscribing to that stream API so nowlet's go ahead and enable our producerin order to publish this data or theseevents or this information to our Kafkabroker and to do that we just need to goback here and here instead of logginglet's say for example producer and thensend message so we can use this thismessage uh this method reference sincethe sent message also takes a string asa parameter also you can reenable thelogs right here but this should be alsofine now if we restart the applicationand send we should be able to have thisthe data published to our kfka so let'skeep it for later on when we run thewhole application or you can also let'slet's also give it a try so here if Iopen back my terminal right here and Itry to consume the topic that we have orthe topic that we created which we whichwe called our weeky media and then Dashstream all right so I will just hitenter here in order to have the streamready and now I will go back to ourPostman and just hit enter and here I'mexpecting to see the data startingpopping up all right so my bad I justrest started the wrong application so Iwill stop this one one and then we clickon service again so here we have ourproducer so this one it's alreadystopped and let's restart ourproducer all right so the producer is upand running and now if I open back theterminal and I click on send again sothis one is running and I'm expecting tosee a data right here so this is what weare seeing here and if I scroll down theconsumer will keep consuming the data sonow like the good think we aresuccessfully publishing data and so farwe published 401 messages okay so now sothe producer is up and ready let's moveon and create and Implement our consumerand have the application fully up andrunning all right now let's go ahead anddo the same thing with our consumer sothe first thing we need to do I want torename this one or change the extensionto yaml instead of properties you canalso use properties if you feel muchcomfortable with that and again I canalso copy these properties from here solet me copy all this configuration forour consumer and paste it in here so nowI just want to bring back that I want toconsume my string or like I want to usethe string der serializer instead of theJson der serializer also here we caneven remove the this one since it's notneeded because we are just consumingstring messages so but I will leave itin the com commented here so you can useit later on and also this is our groupID you can change the name if you wantto use a new one and here we can keepthe same configuration and of course ourbootstrap server is also the same so thenext step is let's go ahead also andcopy the configuration from our producerthe one that we use to configure ourtopic so here just we need this oneWikipedia topic config and let's alsobring it here so here you might say okaybut this is a duplication the answer isnot this is not duplication because it'sjust duplication when you duplicate thecode but otherwise this one is not aduplication because these two servicesare supposed to be separate Services allright so here I will also createuh just my consumer package and thenWikipedia consumer and I will come backto it just in a few seconds also I wantto move this uh Wikimedia topic configso like uh there is a cool way to do itwith an anj so here within the packagename you can just add do config at theend and then anj will propose to movethis package so here if you use the autocompletion and it will ask you to movethe package to this new package allright so here all right so now we justmoved this configuration there now Iwill close all this and here like theconsumer part is the easiest one so allwe need to do is to create our consumerand in order to do that we can also goback to our cfad demo and we can copythe code from there just not to wastetime on something that we already sawtogether before so so now all they needto do is to copy this code right hereand let's go back to our consumer andhere let's make this one service andthen also I'm going to use slf for J andI will paste this one and reenable thisannotation all right so it's a Kafkalistener and here we need to use thesame topic exact so let's close thisdemo right here and let's go back sothis is our our topic name which isWikipedia Stream So now going back toour consumer let's give it a name andalso if you changed the group ID in theconfiguration please make sure that youchange it in here so now let's sayconsuming the message from Wikipediastream topic and then let's also log themessage here so now that's it normallythis is all we need to do for ourconsumer because we just want to consumeand also here please I'm going I'm goingalso to leave it as a comment so pleasefeel free to do anything you want withthedata all right so I would just leave itto you and as I mentioned before I wouldjust I would like you to implementsomething and also share the links uhthe GitHub links with me and I would behappy to do a code review for you allright now what we need to do next weneed to start the two applications andsee once we start consuming or once westart producing and pushing data to ourbroker we need to make sure that we arealso consuming it from this part allright so now so now the first thing thatI will be doing is to start my consumerso this is the consumer application Iwill start it in debug mode and see ifeverything is fine so now let's sayLet's see we have alreadycons we are consuming already some dataso this is because we have already somecued messages now if I start my producerapplication so let's start the producerand then let's just invoke the endpointand then we will seethat okay so yeah that's that's fine weare not able to start the producerbecause both applications are running onthe same port so let's go to our prodproducerand go to application Properties orapplication yaml and here at the endlet's say server. port and let's say8081 for example all right so now I willrestart the producer and now it shouldbe up and running normally without anyissues okay so the producer is alsorunning I will clean the console andthen I will open again Postman and hereI need to change the board and sendagain the request so here we see that weare or we start producing the data andif I open the consumer we will see thatwe are consuming messages so this iswhat we see right here so the consumeris up and running and here we areconsuming the data coming from ourproducer now if I stop the producer fromsending the data you will see that theconsumer will no longer receive anyother messages so it stop printing thedata that was it for today's video Ihope you enjoyed it I hope you liked itand I hope you learned from it now ifyou're satisfied with the video I wouldlike to invite you to do few thingsfirst if you're not subscribed just goahead and hit the Subscribe buttonenable the ring bell so you will nolonger miss any of my coming videos I'mreally preparing a really nice andexciting content for you guys then ifyou don't know my website just go aheadcheck the alibo coding.com website checkthe courses that I have there and enjoylearning finally if you are notconnected with me on social media justgo ahead and do it you will find all thelinks in the description below of thisvideo thank you so much for watching andsee you next time


